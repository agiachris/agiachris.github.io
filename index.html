<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Christopher Agia">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Chris Agia - Robotics and Learning</title>

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="stylesheet" type="text/css" href="main.css">
    <link rel="icon" type="image/png" href="images/ml_icon.png">

    <style>
      body {
        background-color: #f9f9f9;
      }
    </style>

    <style>
      .representative {
        background-color: #e0ffe0;
      }
    </style>

    <style>
      .accordion {
        background-color: #eee;
        color: #444;
        cursor: pointer;
        padding: 14px;
        width: 100%;
        border: none;
        text-align: right;
        outline: none;
        font-size: 22px;
        font-family: monospace;
        transition: 0.4s;
      }

      .active, .accordion:hover {
        background-color: #ccc;
      }

      .panel {
        padding: 0 0px;
        width:100%;
        vertical-align:middle;
        background-color: #f9f9f9;
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.4s ease-out;
      }
      
      /* Style the button that is used to open and close the collapsible content */
      .collapsible {
        background-color: #f9f9f9;
        /* color: #444; */
        cursor: pointer;
        /* padding: 18px; */
        width: 100%;
        border: none;
        text-align: right;
        outline: none;
        /* font-size: 15px; */
        font-family: monospace;
        transition: 0.4s;
      }

      /* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
      .active, .collapsible:hover {
        background-color: #ccc;
      }

      /* Style the collapsible content. Note: hidden by default */
      .content {
        /* padding: 0 18px; */
        display: none;
        overflow: hidden;
        background-color: #f9f9f9;
        transition: max-height 0.4s ease-out;
      }
    </style>  
  </head>

  <body>

    <table style="width:100%;max-width:875px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
      <tr style="padding:0px">
      <td style="padding:0px">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr style="padding:0px">

            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/headshots/chrisagia_robotshot_crop.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/headshots/chrisagia_robotshot_crop.jpg" class="hoverZoomLink"></a>
            </td>

            <td style="padding:2.5%;width:63%;vertical-align:middle;background-color: #f9f9f9">
              <p style="text-align:center">
                <name>Chris Agia</name>
              </p>
              <p>
                <strong>I am a final year PhD candidate in Computer Science</strong> at Stanford University advised by <a href="https://web.stanford.edu/~bohg/"><strong>Jeannette Bohg</strong></a>
                and <a href="https://web.stanford.edu/~pavone/"><strong>Marco Pavone</strong></a>. I'm a member of the 
                <a href="https://ai.stanford.edu/">Stanford Artificial Intelligence Laboratory</a>, the <a href="https://crfm.stanford.edu/">Center for Research on Foundation Models</a>,
                and work jointly with the <a href="http://iprl.stanford.edu/">Interactive Perception and Robot Learning Lab</a> and the <a href="https://stanfordasl.github.io/">Autonomous Systems Lab</a>.
              </p>
              <p>My research is structured around the following core thrusts, with a particular focus on systems that operate in the physical world (robots):</p>
              <ol> 
                <!-- <li>Increasing the reliability of AI-driven robot during deployment;</li> 
                <li>Establishing connections between model behavior and data;</li> 
                <li>Endowing robots with the ability to plan for a wide range of tasks;</li> 
                <li>Facilitating effective interaction between robots and humans.</li> -->
                <li>Increasing the reliability of AI-driven systems during deployment;</li> 
                <li>Establishing connections between model behavior and data;</li> 
                <li>Expanding full-stack capabilities through planning and reasoning;</li>
                <li>Facilitating effective interaction between systems and humans.</li>
              </ol>
              <!-- <p>
                <strong>My research investigates how robots can feasibly and efficiently plan for any task through learning.</strong>
                This requires accurate reasoning over the effects of robot actions into the future, at both a physical and abstract level, just as we do. 
                I'm interested in defining <em>the right substrates</em> for geometric-symbolic reasoning, 
                designing tools to construct them from perception and language, and learning to enact plans downstream.
              </p> -->
              <!-- <p>
                <strong>I set aside weekly time for research discussion and mentorship.</strong> If you feel this could be of 
                benefit, please book an open slot <a href="https://calendly.com/agiachris/virtual-research-chat">here</a>.
              </p> -->
              <p>
                Beyond research, I enjoy sports (<a href="https://stanfordclubsports.com/sports/mens-soccer">Stanford Club Men's Soccer</a>) and music!
              </p>

              <button type="button" class="collapsible">..more</button>
              <div class="content">
                <p>
                  I've had the pleasure of working with <a href="https://jiajunwu.com/">Jiajun Wu</a> in my first-year at Stanford.
                  Before that, I graduated from the 
                  <a href="https://orientation.engsci.utoronto.ca/2020/07/08/engsci-majors-robotics-engineering/">Engineering Science, Robotics</a> 
                  program at the University of Toronto, where I was advised by <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a> 
                  at <a href="https://rvl.cs.toronto.edu/">RVL</a> and the <a href="https://vectorinstitute.ai/">Vector Institute</a>. 
                  I've been fortunate to collaborate with <a href="https://liampaull.ca/">Liam Paull</a> at 
                  <a href="https://montrealrobotics.ca/">MILA</a>, <a href="https://www.cim.mcgill.ca/~dmeger/">David Meger</a> 
                  and <a href="http://www.cim.mcgill.ca/~dudek/">Gregory Dudek</a> at <a href="https://www.cim.mcgill.ca/~mrl/">McGill</a>, 
                  <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Goldie Nejat</a> at the 
                  <a href="http://asblab.mie.utoronto.ca/">University of Toronto</a>, and colleagues from 
                  <a href="https://www.microsoft.com/en-us/research/group/reinforcement-learning-redmond/">Microsoft Research</a> and 
                  <a href="https://ai.facebook.com/">Meta AI Research</a>. 
                  In industry, I had the opportunity to work on multi-agent reinforcement learning in mixed reality environments at 
                  <a href="https://www.microsoft.com/en-us/research/project/robotics-and-mixed-reality/">Microsoft</a>, perception 
                  and localization for self-driving vehicles at <a href="http://www.noahlab.com.hk/#/home">Huawei Noah's Ark Lab</a> with
                  <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>, and 
                  language-agnostic ABI simulators at <a href="https://about.google/">Google</a> Cloud.
                </p>
              </div>

              <p style="text-align:center">
                cagia[at]cs.stanford.edu &nbsp/&nbsp
                <a href="data/Agia-CV-2025.pdf">CV</a> &nbsp/&nbsp
                <a href="http://www.linkedin.com/in/agiachris/"> LinkedIn</a> &nbsp/&nbsp
                <a href="https://x.com/agiachris"> Twitter/X</a> &nbsp/&nbsp
                <a href="https://github.com/agiachris/"> GitHub</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=t8Em5FwAAAAJ&hl=en">Scholar</a>
              </p>

            </td>
          </tr>
          </tbody>
        </table>
        <p></p>

        <!-- Updates Section -->
        <table style="width:100%;max-width:875px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px;">
          <tbody>
            <tr>
              <td style="padding:0px">
                
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                  <tbody>
                    <tr style="padding:0px">
                      <td style="text-align:right;background-color:#eee;padding:10px;">
                        <heading style="font-family:monospace;font-size:16px;color:#444;">..Updates</heading>
                      </td>
                    </tr>
                  </tbody>
                </table>
                
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color:#f9f9f9;">
                  <tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle;background-color:#f9f9f9;">
                        <div style="height:125px;overflow-y:scroll;padding:10px;background-color:#f9f9f9;">
                          <ul style="list-style-type:disc;padding-left:20px;margin:0;font-family:Lato;font-size:14px;color:#444;">
                            <li><strong><em>[Aug 2025]</em></strong> —- Hosting a full-day workshop on <a href="https://sites.google.com/stanford.edu/corldata25/home">Making Sense of Data in Robotics</a> at CoRL 2025 — join the conversation!</li>
                            <li><strong><em>[Aug 2025]</em></strong> -- Our three recent research projects – <a href="https://cupid-curation.github.io/">CUPID</a>, <a href="https://robomonkey-vla.github.io/">RoboMonkey</a> , and <a href="https://milanganai.github.io/fortress/">FORTRESS</a> – were accepted to CoRL 2025!</li>
                            <li><strong><em>[Jun 2025]</em></strong> -- Our work on robot data curation with influence functions (<a href="https://cupid-curation.github.io/">CUPID</a>) received a best paper award at RSS <a href="https://sites.google.com/stanford.edu/robot-evaluation-rss-2025/home">RoboEval</a>!</li>
                            <li><strong><em>[Jun 2025]</em></strong> -- Three new papers out! <a href="https://cupid-curation.github.io/">CUPID</a> (data curation), <a href="https://robomonkey-vla.github.io/">RoboMonkey</a>  (test-time scaling), and <a href="https://milanganai.github.io/fortress/">FORTRESS</a> (OOD safety)!</li>
                            <li><strong><em>[Jan 2025]</em></strong> -- Offer accepted! I will be joining <a href="https://ai.meta.com/research/">Meta Fundamental AI Research (FAIR)</a> as a Research Scientist Intern this summer!</li>
                            <li><strong><em>[Nov 2024]</em></strong> -- I'm on the Summer 2025 research internship market; please reach out if my research is of interest to your group!</li>
                            <li><strong><em>[Nov 2024]</em></strong> -- Our lab (<a href="https://iprl.stanford.edu/">IPRL</a>) made the <a href="https://engineering.stanford.edu/news/meet-few-roboticists-designing-tomorrows-technology-new-robotics-center?utm_source=mailchimp&utm_medium=email&utm_campaign=issue28rec&utm_content=meet-a-few-of-the-roboticists">front page</a> of the Stanford School of Engineering news for the Stanford Robotics Center!</li>
                            <li><strong><em>[Sep 2024]</em></strong> -- I've been featured on the <a href="https://stanforddaily.com/2024/09/26/stanford-center-autonomous-space-ventures/">Stanford Daily News</a> for my perspectives on foundation models for space applications!</li>
                            <li><strong><em>[Sep 2024]</em></strong> -- Our work on <a href="https://sites.google.com/stanford.edu/sentinel">detecting policy failures</a> was accepted to CoRL 2024 (oral presentation at the <a href="https://sites.google.com/view/corl-2024-safe-rol-workshop/home">SAFE-ROL</a> workshop)!</li>
                            <li><strong><em>[Sep 2024]</em></strong> -- Our work on <a href="https://sites.google.com/view/text2interaction">human-robot interactive planning</a> with large language models was accepted to CoRL 2024!</li>
                            <li><strong><em>[Jul 2024]</em></strong> -- Our paper on <a href="https://sites.google.com/view/aesop-llm">real-time anomaly detection</a> with language models received the outstanding paper award at RSS!</li>
                            <li><strong><em>[May 2024]</em></strong> -- Two of our papers have been accepted to RSS 2024: Check out <a href="https://sites.google.com/view/aesop-llm">AESOP</a> and <a href="https://droid-dataset.github.io/">DROID</a>!</li>
                            <li><strong><em>[Jan 2024]</em></strong> -- My internship research on <a href="https://sites.google.com/stanford.edu/spacecraft-models">autonomous space exploration</a> has been accepted to IEEE AeroConf 2024!</li>
                            <li><strong><em>[Jan 2024]</em></strong> -- Our paper on <a href="https://robotics-transformer-x.github.io/">Open X-Embodiment</a> learning has been accepted to ICRA 2024 with a best paper award nomination!</li>
                            <li><strong><em>[Jul 2023]</em></strong> -- We have two <a href="https://sites.google.com/stanford.edu/text2motion">journal</a> <a href="https://sites.google.com/view/llm-anomaly-detection/home">papers</a> accepted to <a href="https://link.springer.com/collections/bejhchcbag">Autonomous Robots, Special Issue: Large Language Models in Robotics</a>!</li>
                            <li><strong><em>[Jun 2023]</em></strong> -- I'm excited to join the <a href="https://www.jpl.nasa.gov/">NASA Jet Propulsion Laboratory</a> for a robotics research internship this summer!</li>
                            <li><strong><em>[Jan 2023]</em></strong> -- Our work on <a href="https://sites.google.com/stanford.edu/stap/home">sequencing policies</a> for long-horizon manipulation has been accepted to ICRA 2023!</li>
                          </ul>
                        </div>
                      </td>
                    </tr>
                  </tbody>
                </table>

              </td>
            </tr>
          </tbody>
        </table>


        <!-- Education -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr>
            <!-- <td style="text-align: right; background-color: #eee"> -->
            <td style="text-align: right; background-color: #eee; padding:14px;">
              <heading style="font-family: monospace; font-size: 22px; color: #444">..Education</heading>
            </td>
          </tr>
          </tbody>
        </table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>   -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;">
          <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/school/Stanford-Crest-Square.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Ph.D in Computer Science</papertitle><br>
              Department of Computer Science, Stanford University<br>
              Sep 2021 | Stanford, CA
              <p></p> 
              <em><strong>School of Engineering Fellowship</strong></em><br>
              <!-- <em><strong>Teaching assistant: Principles of Robot Autonomy 1 (2x) and 2 (1x)</strong></em> -->
              <em><strong>2 x TA for Principles of Robot Autonomy 1</strong> (<a href="https://stanfordasl.github.io/PoRA-I/aa174a_aut2324/">2023</a>, <a href="https://stanfordasl.github.io/PoRA-I/aa174a_aut2425/">2024</a>)</em><br>
              <em><strong>1 x TA for Principles of Robot Autonomy 2</strong> (<a href="https://web.stanford.edu/class/cs237b/">2025</a>)</em><br>
            </td>
          </tr>
          </tbody>
        </table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>   -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;">
          <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/school/UofT-Crest-Square.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>B.A.Sc in Engineering Science, Robotics</papertitle><br>
              Faculty of Applied Science and Engineering, University of Toronto<br>
              Sep 2016 - May 2021 | Toronto, ON<br>
              <p></p> 
              <em><strong>President's Scholarship Program</strong></em><br>
              <em><strong>NSERC Undergraduate Research Award</strong></em><br>
              <em><strong>Dean's Honour List - 2018-2021</strong></em>
            </td>
          </tr>
        </tbody></table>
        </div>
        <p></p>

        <!-- Research -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="text-align: right; background-color: #eee; padding:14px;">
              <heading style="font-family: monospace; font-size: 22px; color: #444">..Research</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p style="font-size: 16px">
                Papers and preprints are ordered by recency. Representative works are <span class="representative">highlighted in green</span>.
              </p>
            </td>
          </tr>
        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Preprints</heading>
            </td>
          </tr>
        </tbody></table> -->

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Journal Papers</heading>
            </td>
          </tr>
        </tbody></table> -->

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Conference Papers</heading>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding-top:12.5px;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" src="images/research/robopair.mp4" type="video/mp4" muted autoplay loop></video>
              <video width="140" src="images/research/jdapt.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>Preventing Robotic Jailbreaks via Multimodal Domain Adaptation</papertitle>
              <br>
              <a href="https://www.math.unipd.it/~fmarchio/">Francesco Marchiori</a>, <a href="https://rohansinha.nl/">Rohan Sinha</a>, <strong>Christopher Agia</strong>, <a href="https://arobey1.github.io/">Alexander Robey</a>, <a href="https://www.georgejpappas.org/">George Pappas</a>, <a href="https://www.math.unipd.it/~conti/">Mauro Conti</a>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>
              <br>
              <em>arXiv preprint</em>
              <br>
              <a href="https://arxiv.org/abs/2509.23281">arXiv</a> / <a href="https://j-dapt.github.io">Project Site</a>
              <br>
              <p>
                LLMs and VLMs are increasingly deployed in robotics but remain vulnerable to jailbreaking attacks that may drive physically harmful behaviors in the real world.
                We introduce J-DAPT, a real-time robotics jailbreak detector that adapts to prevent novel threats in data-limited regimes via multimodal domain adaptation.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table class="representative" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding-top:12.5px;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" src="images/research/cupid.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>CUPID: Curating Data your Robot Loves with Influence Functions</papertitle>
              <br>
              <strong>Christopher Agia</strong>, <a href="https://rohansinha.nl/">Rohan Sinha</a>, <a href="https://yjy0625.github.io/">Jingyun Yang</a>, <a href="https://contactrika.github.io/">Rika Antonova</a>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>, <a href="https://harukins.github.io/">Haruki Nishimura</a>, <a href="https://mashaitkina.weebly.com/">Masha Itkina</a>, <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2025 | Seoul, South Korea
              <br>
              <a href="https://arxiv.org/abs/2506.19121">arXiv</a> / <a href="https://cupid-curation.github.io/">Project Site</a> / <a href="https://www.youtube.com/watch?v=0kmIiEjTqV8">YouTube</a> / <a href="https://github.com/agiachris/cupid">Code</a> / <a href="https://x.com/agiachris/status/1937670321458606310">X</a>
              <br>
              <span style="font-size:13px;">RSS 2025 Workshop on Robot Evaluation for the Real World (<span style="color:red;">Best Paper</span>)</span>
              <p>
                In robot imitation learning, policy performance is tightly coupled with the quality and composition of the demonstration data. 
                We present CUPID, a data curation method that uses influence functions to estimate the causal impact of each demonstration on a policy's closed-loop performance. 
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;padding-top:12.5px;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" src="images/research/robomonkey.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models</papertitle>
              <br>
              <a href="https://www.linkedin.com/in/jackykwok02/">Jacky Kwok</a>, <strong>Christopher Agia</strong>, <a href="https://rohansinha.nl/">Rohan Sinha</a>, <a href="https://www.linkedin.com/in/matthew-foutter-7b1366192/">Matt Foutter</a>, <a href="https://shilun-allan-li.github.io/">Shulu Li</a>, <a href="https://people.eecs.berkeley.edu/~istoica/">Ion Stoica</a>, <a href="http://azaliamirhoseini.com/">Azalia Mirhoseini</a>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2025 | Seoul, South Korea
              <br>
              <a href="https://arxiv.org/abs/2506.17811">arXiv</a> / <a href="https://robomonkey-vla.github.io/">Project Site</a> / <a href="https://github.com/robomonkey-vla/RoboMonkey">Code</a>  / <a href="https://x.com/jackyk02/status/1943013062170284368">X</a>
              <p>
                Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in visuomotor control, yet ensuring their robustness in unstructured real-world environments remains a persistent challenge. 
                In this paper, we investigate inference-time scaling as means to enhance VLA robustness and generalization. 
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;padding-top:12.5px;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" src="images/research/fortress_drone.mp4" type="video/mp4" muted autoplay loop></video>
              <video width="140" src="images/research/fortress_anymal.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning</papertitle>
              <br>
              <a href="https://milanganai.github.io/">Milan Ganai</a>, <a href="https://rohansinha.nl/">Rohan Sinha</a>, <strong>Christopher Agia</strong>, <a href="https://danielpmorton.github.io/">Daniel Morton</a>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2025 (<span style="color:red;">Oral Presentation</span>) | Seoul, South Korea
              <br>
              <a href="https://arxiv.org/abs/2505.10547">arXiv</a> / <a href="https://milanganai.github.io/fortress/">Project Site</a>
              <p>
                Foundation models can reason about appropriate safety interventions in hazardous out-of-distribution scenarios beyond a robot's training data.
                We present FORTRESS, a framework that generates and reasons about semantically safe fallback strategies in real time to prevent out-of-distribution failures.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;padding-top:12.5px;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" src="images/research/p2p_apple.mp4" type="video/mp4" muted autoplay loop></video>
              <video width="140" src="images/research/p2p_shelf.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>Points2Plans: From Point Clouds to Long-Horizon Plans with Composable Relational Dynamics</papertitle>
              <br>
              <a href="https://yixuanhuang98.github.io/">Yixuan Huang</a>, <strong>Christopher Agia</strong>, <a href="https://jimmyyhwu.github.io/">Jimmy Wu</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a>, <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2025 | Atlanta, US
              <br>
              <a href="https://arxiv.org/abs/2408.14769">arXiv</a> / <a href="https://sites.google.com/stanford.edu/points2plans">Project Site</a> / <a href="https://github.com/yixuanhuang98/Points2Plans">Code</a>
              <br>
              <span style="font-size:13px;">CoRL 2024 Workshop on Learning Effective Abstractions for Planning (<span style="color:red;">Oral Presentation</span>)</span>
              <p>
                How can we plan to solve unseen, long-horizon tasks from a single, partial-view point cloud of the scene, and can we do so without access 
                to long-horizon training data? Points2Plans leverages transformer-based relational dynamics to learn the symbolic and geometric effects of 
                robot skills, then compose the skills at test time to generate a long-horizon symbolic and geometric plan.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table class="representative" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding-top:12.5px;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/sentinel_pushchair_ood.gif" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Unpacking Failure Modes of Generative Policies: Runtime Monitoring of Consistency and Progress</papertitle>
              <br>
              <strong>Christopher Agia</strong>, <a href="https://rohansinha.nl/">Rohan Sinha</a>, <a href="https://yjy0625.github.io/">Jingyun Yang</a>, <a href="https://www.linkedin.com/in/zi-ang-cao-7b677a317/">Zi-ang Cao</a>, <a href="https://contactrika.github.io/">Rika Antonova</a>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>, <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2024 | Munich, DE
              <br>
              <a href="https://arxiv.org/abs/2410.04640">arXiv</a> / <a href="https://sites.google.com/stanford.edu/sentinel">Project Site</a> / <a href="https://youtu.be/rmufD7VMivc">YouTube</a> / <a href="https://github.com/agiachris/sentinel">Code</a> / <a href="https://x.com/agiachris/status/1844429583212478665">X</a>
              <br>
              <span style="font-size:13px;">CoRL 2024 Workshop on Safe and Robust Robot Learning (<span style="color:red;">Oral Presentation</span>)</span>
              <br>
              <span style="font-size:13px;">RSS 2024 Workshop: Towards Safe Autonomy: Emerging Requirements, Definitions, and Methods</span>
              <p>
                Robot behavior policies trained via imitation learning are prone to failure under conditions that deviate from their training data.
                In this work, we present Sentinel, a runtime monitor that detects unknown failures (requiring no data of failures) of generative robot policies at deployment time.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;padding-top:12.5px;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/t2i_handover_bad.gif" alt="clean-usnob" width="140">
              <img src="images/research/t2i_handover_good.gif" alt="clean-usnob" width="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Text2Interaction: Establishing Safe and Preferable Human-Robot Interaction</papertitle>
              <br>
              <a href="https://jakob-thumm.com/">Jakob Thumm</a>, <strong>Christopher Agia</strong>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>, <a href="https://scholar.google.de/citations?user=E3zazJAAAAAJ&hl=de">Matthias Althoff</a>
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2024 | Munich, DE
              <br>
              <a href="https://arxiv.org/abs/2408.06105">arXiv</a> / <a href="https://sites.google.com/view/text2interaction">Project Site</a> / <a href="https://www.youtube.com/watch?v=LNfVw9ZtAtI">YouTube</a> / <a href="https://github.com/JakobThumm/text2interaction">Code</a>
              <br>
              <span style="font-size:13px;">CoRL 2024 Workshop on Language and Robot Learning: Language as an Interface</span>
              <p>
                How can we integrate human preferences into robot plans in a zero-shot manner, i.e., without requiring tens of thousands of data points
                of human feedback? We propose Text2Interaction, a planning framework that invokes large language models to generate a task plan, motion preferences as Python code, 
                and parameters of a safe controller.
              </p>
            </td>
          </tr>
        </tbody></table>
        
        <table class="representative" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding-top:12.5px;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" src="images/research/aesop_llm.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>Real-Time Anomaly Detection and Reactive Planning with Large Language Models</papertitle>
              <br>
              <a href="https://stanfordasl.github.io//people/rohan-sinha/">Rohan Sinha</a>, <a href="https://stanfordasl.github.io//people/amine-elhafsi/">Amine Elhafsi</a>, <strong>Christopher Agia</strong>, <a href="https://www.linkedin.com/in/matthew-foutter-7b1366192/">Matt Foutter</a>, <a href="https://stanfordasl.github.io//people/edward-schmerling/">Edward Schmerling</a>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2024 (<span style="color:red;">Outstanding Paper</span>) | Delft, NL
              <br>
              <a href="https://arxiv.org/abs/2407.08735v1">arXiv</a> / <a href="https://sites.google.com/view/aesop-llm">Project Site</a> / <a href="https://www.youtube.com/watch?v=2_XzYYLG2iY&ab_channel=RohanSinha">YouTube</a> / <a href="https://www.youtube.com/watch?v=TSC_mVH5abI&list=PLZHnYvH1qtOYkElUMqYiHDMrGTPnqRhSr&index=2">NVIDIA Media / <a href="https://techxplore.com/news/2024-08-stage-framework-llm-based-anomaly.html">TechXplore</a>
              <br>
              <span style="font-size:13px;">CoRL 2024 Workshop on Language and Robot Learning: Language as an Interface</span>
              <p>
                How can we mitigate the computational expense and latency of LLMs for real-time anomaly detection and reactive planning? 
                We propose a two-stage reasoning framework, whereby fast a LLM embedding model flags potential observational anomalies
                while a slower generative LLM assesses the safety-criticality of flagged anomalies and selects a safety-preserving fallback plan.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;padding-top:12.5px;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" height="80" src="images/research/cooking_droid_speed.mp4" type="video/mp4" muted autoplay loop></video>
              <video width="140" height="80" src="images/research/apple_droid.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset</papertitle>
              <br>
              <a href="https://github.com/AlexanderKhazatsky">Alexander Khazatsky*</a>, <a href="https://kpertsch.github.io/">Karl Pertsch*</a>, ..., <strong>Christopher Agia</strong>, ..., <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2024 | Delft, NL
              <br>
              <a href="https://arxiv.org/abs/2403.12945">arXiv</a> / <a href="https://droid-dataset.github.io/">Project Site</a> / <a href="https://github.com/droid-dataset/droid">Code</a>
              <p>We introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with
                76k demonstration trajectories (or 350 hours of interaction data), collected across 564 scenes and 86 
                tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate
                that training with DROID leads to policies with higher performance and improved generalization ability.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;padding-top:12.5px;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" height="80" src="images/research/dare_mission_concept.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>Modeling Considerations for Developing Deep Space Autonomous Spacecraft and Simulators</papertitle>
              <br>
              <strong>Christopher Agia</strong>, <a href="https://www.guillemc.com/">Guillem Casadesus Vila</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/saptarshi_bandyopadhyay/">Saptarshi Bandyopadhyay</a>, <a href="https://scienceandtechnology.jpl.nasa.gov/people/d_bayard">David S. Bayard</a>, <a href="https://descanso.jpl.nasa.gov/biography/cheung.html">Kar-Ming Cheung</a>, <a href="https://ieeexplore.ieee.org/author/37336718000">Charles H. Lee</a>, <a href="https://ieeexplore.ieee.org/author/37285842700">Eric Wood</a>, <a href="https://www.linkedin.com/in/ian-aenishanslin-a46b61175/?originalSubdomain=fr">Ian Aenishanslin</a>, <a href="https://www.linkedin.com/in/steven-ardito-08a705b/">Steven Ardito</a>, <a href="https://www.cs.ucla.edu/lorraine-fesq/">Lorraine Fesq</a>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/issa_nesnas/">Issa A. D. Nesnas</a>
              <br>
              <em>IEEE Aerospace Conference (AeroConf)</em>, 2024 | Montana, US
              <br>
              <a href="papers/Spacecraft-Models-AeroConf-2024.pdf">PDF</a> / <a href="https://arxiv.org/abs/2401.11371">arXiv</a> / <a href="https://sites.google.com/stanford.edu/spacecraft-models">Project Site</a> / <a href="https://youtu.be/ejP_IDua6J0">Concept of Operations (Video)</a>
              <p>
                Future space exploration missions to unknown worlds will require robust reasoning, planning, and decision-making capabilities,
                enabled by the right choice of onboard models. In this work, we aim to understand what onboard models a spacecraft needs for fully autonomous space exploration.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;padding-top:12.5px;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" height="80" src="images/research/openx_teaser.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>Open X-Embodiment: Robotic Learning Datasets and RT-X Models</papertitle>
              <br>
              Open X-Embodiment Collaboration
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024 (<span style="color:red;">Best Paper</span>) | Yokohama, JP
              <br>
              <a href="https://arxiv.org/abs/2310.08864">arXiv</a> / <a href="https://robotics-transformer-x.github.io/">Project Site</a> / <a href="https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/">Blogpost</a> / <a href="https://github.com/google-deepmind/open_x_embodiment">Code</a>
              <p>Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. 
                Can we instead train “generalist” X-robot policy that can be adapted efficiently to new robots, tasks, and environments?
              </p>
            </td>
          </tr>
        </tbody></table>

        <table class="representative" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding-top:12.5px;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/t2m_task6.gif" alt="clean-usnob" width="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Text2Motion: From Natural Language Instructions to Feasible Plans</papertitle>
              <br>
              <a href="https://kevin-thankyou-lin.github.io/">Kevin Lin*</a>, <strong>Christopher Agia*</strong>, <a href="https://cs.stanford.edu/~takatoki/">Toki Migimatsu</a>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>, <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>
              <br>
              <em>Special Issue: Large Language Models in Robotics, Autonomous Robots (AR)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.12153">arXiv</a> / <a href="https://link.springer.com/article/10.1007/s10514-023-10131-7">Springer Nature</a> / <a href="https://sites.google.com/stanford.edu/text2motion">Project Site</a> / <a href="https://youtu.be/JdzKskGS5Qo">YouTube</a> / <a href="https://www.youtube.com/watch?v=DNPeBznhBD4">Invited Talk</a>
              <br>
              <span style="font-size:13px;">ICRA 2023 Workshop on Pretraining for Robotics</span>
              <p>Pretrained large language models can be readily used to obtain high-level robot plans from natural lanugage instructions, but should these
                plans be executed without verifying them on the geometric-level? We propose Text2Motion, a language-based planner that tests if 
                LLM-generated plans (a) satisfy user instructions and (b) are geometric feasibility prior to executing them.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;padding-top:12.5px;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/semantic_anomaly.png" alt="clean-usnob" width="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Semantic Anomaly Detection with Large Language Models</papertitle>
              <br>
              <a href="https://stanfordasl.github.io//people/amine-elhafsi/">Amine Elhafsi</a>, <a href="https://stanfordasl.github.io//people/rohan-sinha/">Rohan Sinha</a>, <strong>Christopher Agia</strong>, <a href="https://stanfordasl.github.io//people/edward-schmerling/">Edward Schmerling</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/issa_nesnas/">Issa A. D. Nesnas</a>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>
              <br>
              <em>Special Issue: Large Language Models in Robotics, Autonomous Robots (AR)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2305.11307">arXiv</a> / <a href="https://link.springer.com/article/10.1007/s10514-023-10132-6">Springer Nature</a> / <a href="https://sites.google.com/view/llm-anomaly-detection/home">Project Site</a>
              <p>System-level failures are not due to failures of any individual component of the autonomy stack but
                system-level deficiencies in semantic reasoning. Such edge cases, dubbed semantic anomalies, 
                are simple for a human to disentangle yet require insightful reasoning. We introduce a runtime monitor based 
                on large language models to recognize failure-inducing semantic anomalies.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table class="representative" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding-top:12.5px;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" height="80" src="images/research/stap_hook_reach.mp4" type="video/mp4" muted autoplay loop></video>
              <video width="140" height="80" src="images/research/stap_constrained_packing.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>STAP: Sequencing Task-Agnostic Policies</papertitle>
              <br>
              <strong>Christopher Agia*</strong>, <a href="https://cs.stanford.edu/~takatoki/">Toki Migimatsu*</a>, <a href="https://jiajunwu.com/">Jiajun Wu</a>, <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2023 | London, UK
              <br>
              <a href="papers/STAP-ICRA-2023.pdf">PDF</a> / <a href="https://arxiv.org/abs/2210.12250">arXiv</a> / <a href="https://sites.google.com/stanford.edu/stap">Project Site</a> / <a  href="https://youtu.be/78HiaQ6oF4s">YouTube</a> / <a href="https://github.com/agiachris/STAP">Code</a>
              <p>Solving sequential manipulation tasks requires coordinating geometric dependencies between actions. 
                We develop a scalable framework for training skills independently, and then combine the skills at planning
                time to solve unseen long-horizon tasks. Planning is formulated as a maximization problem over the expected 
                success of the skill sequence, which we demonstrate is well-approximated by the product of Q-values.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table class="representative" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding-top:12.5px;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/taskography_3dsg.png" alt="clean-usnob" width="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Taskography: Evaluating Robot Task Planning over Large 3D Scene Graphs</papertitle>
              <br>
              <strong>Christopher Agia*</strong>, <a href="https://krrish94.github.io/author/krishna-murthy-jatavallabhula/">Krishna Murthy Jatavallabhula*</a>, <a href="https://www.linkedin.com/in/khodeir/?originalSubdomain=ca">Mohamed Khodeir</a>, <a href="https://www.microsoft.com/en-us/research/people/onmiksik/">Ondrej Miksik</a>, <a href="http://www.mustafamukadam.com/">Mustafa Mukadam</a>, <a href="http://vibhavvineet.info/">Vibhav Vineet</a>, <a href="https://liampaull.ca/">Liam Paull</a>, <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a>
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2021 | London, UK
              <br>
              <a href="papers/Taskography-CoRL-2021.pdf">PDF</a> / <a href="https://arxiv.org/abs/2207.05006">arXiv</a> / <a href="https://taskography.github.io/">Project Site</a> / <a href="https://github.com/taskography">Code</a>
              <p>3D Scene Graphs (3DSGs) are informative abstractions of our world that unify symbolic, semantic, and metric scene representations. 
                We present a benchmark for robot task planning over large 3DSGs and evaluate classical and learning-based planners; 
                showing that real-time planning requires 3DSGs and planners to be jointly adapted to better exploit 3DSG hierarchies.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;padding-top:12.5px;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/aug_att_rl.gif" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Latent Attention Augmentation for Robust Autonomous Driving Policies</papertitle>
              <br>
              <a href="https://rancheng.github.io/about/">Ran Cheng*</a>, <strong>Christopher Agia*</strong>, <a href="https://www.cim.mcgill.ca/~dmeger/">David Meger</a>, <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a>, <a href="http://www.cim.mcgill.ca/~dudek/">Gregory Dudek</a>
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2021 | Prague, CZ
              <br>
              <a href="papers/AttAugDRL-Self-Driving-IROS-2021.pdf">PDF</a> / <a href="https://ieeexplore.ieee.org/document/9636449">IEEExplore</a>
              <p>Pretraining visual representations for robotic reinforcement learning can improve sample efficiency and policy performance. 
                In this paper, we take an alternate approach and propose to augment the state embeddings of a self-driving agent with 
                attention in the latent space, accelerating the convergence of Actor-Critic algorithms.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;padding-top:12.5px;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" height="140" src="images/research/sim2real_drl_rough_nav.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>A Sim-to-Real Pipeline for Deep Reinforcement Learning for Autonomous Robot Navigation in Cluttered Rough Terrain</papertitle>
              <br>
              <a href="https://www.linkedin.com/in/richardhu-12dsf/?originalSubdomain=ca">Han Hu*</a>, <a href="https://www.linkedin.com/in/kczhang/?originalSubdomain=ca">Kaicheng Zhang*</a>, <a href="https://www.linkedin.com/in/aaron-hao-tan/?originalSubdomain=ca">Aaron Hao Tan</a>, <a href="https://www.linkedin.com/in/michael-ruan-0822/">Michael Ruan</a>, <strong>Christopher Agia</strong>, <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/"> Goldie Nejat</a>
              <br>
              <em>IEEE Robotics and Automation Letters (RA-L)</em> at <em>IROS</em>, 2021 | Prague, CZ
              <br>
              <a href="papers/Sim2RealDRLNav-RAL-2021.pdf">PDF</a> / <a href="https://ieeexplore.ieee.org/document/9468918">IEEExplore</a> / <a href="https://www.youtube.com/watch?v=dtYlNWvK-7k&ab_channel=AutonomousSystemsandBiomechatronicsLab%28UniversityofToronto%29">Demo</a>
              <p>Deep Reinforcement Learning is effective for learning robot navigation policies in rough terrain and cluttered simulated environments. 
                In this work, we introduce a series of techniques that are applied in the policy learning phase to enhance transferability to real-world domains.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;padding-top:12.5px;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/sem_loc.gif" alt="clean-usnob" width="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Lightweight Semantic-aided Localization with Spinning LiDAR Sensor</papertitle>
              <br>
              Yuan Ren, <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>, <a href="https://rancheng.github.io/about/">Ran Cheng</a>, <strong>Christopher Agia</strong>
              <br>
              <em>IEEE Transactions on Intelligent Vehicles (T-IV)</em>, 2021
              <br>
              <a href="papers/SemanticLoc-TIV-2021.pdf">PDF</a> / <a href="https://ieeexplore.ieee.org/document/9495210">IEEExplore</a>
              <p>How can semantic information be leveraged to improve localization accuracy in changing environments? We present a robust LiDAR-based localization 
                algorithm that exploits both semantic and geometric properties of the scene with an adaptive fusion strategy.</p>
            </td>
          </tr>
        </tbody></table>

        <table class="representative" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding-top:12.5px;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/s3cnet_demo.gif" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point Clouds</papertitle>
              <br>
              <a href="https://rancheng.github.io/about/">Ran Cheng*</a>, <strong>Christopher Agia*</strong>, Yuan Ren, <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2020 | Cambridge, US
              <br> 
              <a href="papers/S3CNet-CoRL-2020.pdf">PDF</a> / <a href="https://arxiv.org/abs/2012.09242">arXiv</a> / <a href="https://www.youtube.com/watch?v=ircJBFc5PqM&feature=youtu.be&ab_channel=CoRLConference">YouTube</a> / <a href="https://www.youtube.com/watch?v=voU_zAhNDnQ&feature=youtu.be&ab_channel=RanCheng">Demo</a>
              <p>Small-scale semantic reconstruction methods have had little success in large outdoor scenes as a result of exponential increases in sparsity, 
                and a computationally expensive design. We propose a sparse convolutional network architecture based on the <a href="https://github.com/StanfordVL/MinkowskiEngine">Minkowski Engine</a>, 
                achieving state-of-the-art results for semantic scene completion in 2D/3D space from LiDAR point clouds.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;padding-top:12.5px;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/ddvo_res.gif" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Depth Prediction for Monocular Direct Visual Odometry</papertitle>
              <br>
              <a href="https://rancheng.github.io/about/">Ran Cheng</a>, <strong>Christopher Agia</strong>, <a href="https://www.cim.mcgill.ca/~dmeger/">David Meger</a>, <a href="http://www.cim.mcgill.ca/~dudek/">Gregory Dudek</a>
              <br>
              <em>Conference on Computer and Robotic Vision (CRV)</em>, 2020 | Ottawa, CA
              <br> 
              <a href="papers/CNN-DVO-CRV-2020.pdf">PDF</a> / <a href="https://ieeexplore.ieee.org/document/9108693">IEEExplore</a> / <a href="https://www.youtube.com/watch?v=Z8vpet0doik&feature=youtu.be&ab_channel=ComputerRobotVision">YouTube</a>
              <p>Direct methods are able to track motion with considerable long-term accuracy. However, scale inconsistent estimates arise from random or unit depth initialization. 
                We integrate dense depth prediction with the Direct Sparse Odometry system to accelerate convergence in the windowed bundle-adjustment and promote estimates with consistent scale.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Theses</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/thesis_3dsg.gif" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Contextual Graph Representations for Task-driven 3D Perception and Planning</papertitle>
              <br>
              <strong>Christopher Agia</strong>, <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a>
              <br>
              <em>Division of Engineering Science, University of Toronto</em>, 2020 | Toronto, CA
              <br>
              <a href="https://drive.google.com/file/d/1LjTdgwuiJa-gIiVbbqj9vh-qoEZgqkb_/view?usp=sharing">PDF</a>
              <p>We evaluate the suitability of existing simulators for research at the intersection of task planning and 3D scene graphs and
                construct a benchmark for comparison of symbolic planners. Furthermore, we explore the use of Graph Neural Networks to
                harness invariances in the relational structure of planning domains and learn representations that afford faster planning.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Patents</heading>
              <p>Several components of my industry research projects were patented alongside submitting to conference / journal venues.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/patent_road_segmentation.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Systems and Methods for Generating a Road Surface Semantic Segmentation Map from a Sequence of Point Clouds</papertitle>
              <br>
              <strong>Christopher Agia</strong>, <a href="https://rancheng.github.io/about/">Ran Cheng</a>, Yuan Ren, <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>
              <br>
              Application No. 17/676,131. Patent No. 12,008,762. U.S. Patent and Trademark Office, 2022
              <br>
              <a href="https://patents.google.com/patent/US12008762B2/en">Google Patents</a>
              <p>Relates to processing point clouds for autonomous driving of a vehicle. More specifically, relates to processing a sequence of point 
                clouds to generate a birds-eye-view (BEV) image of an environment of the vehicle which includes pixels associated with road surface labels.</p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/patent_scene_completion.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Methods and Systems for Semantic Scene Completion for Sparse 3D Data</papertitle>
              <br>
              <a href="https://rancheng.github.io/about/">Ran Cheng*</a>, <strong>Christopher Agia*</strong>, Yuan Ren, <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>
              <br>
              Application No. 17/492,261. Patent No. 12,079,970. U.S. Patent and Trademark Office, 2022
              <br>
              <a href="https://patents.google.com/patent/US12079970B2/en">Google Patents</a>
              <p>Relates to methods and systems for generating semantically completed 3D data from sparse 3D data such as point clouds.</p>
            </td>
          </tr>
        </tbody></table>
        <p></p>

        
        <!-- Work Experience -->
        <button class="accordion">..Work Experience</button>
        <div class="panel">
        <p></p>
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work Experience</heading>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/NASA_logo.jpg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Visiting Research Scholar</papertitle>
              <br>
                <a href="https://www.jpl.nasa.gov/">NASA Jet Propulsion Laboratory</a>, <a href="https://www-robotics.jpl.nasa.gov/">Mobility and Robotics Systems</a>
              <br>
                Jun 2023 - Sep 2023 | Pasadena, California
              <p> 
                Research on deep space robotic autonomy.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/microsoft_logo.jpg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Software Engineering Intern</papertitle>
              <br>
                <a href="https://www.microsoft.com/en-us/research/project/robotics-and-mixed-reality/">Microsoft</a>, Mixed Reality and Robotics
              <br>
                May 2021 - Aug 2021 | Redmond, Washington
              <p> 
                Research & development at the intersection of mixed reality, artificial intelligence, and robotics. 
                Created a process unlocking the training and <a href="https://www.microsoft.com/en-us/hololens/hardware">HL2</a> 
                deployment of multi-agent reinforcement learning scenarios in shared digital spatial-semantic representations with 
                <a href="https://docs.microsoft.com/en-us/windows/mixed-reality/design/scene-understanding">Scene Understanding</a>.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/vector_institute_logo.jpg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Robotics & ML Researcher</papertitle>
              <br>
                <a href="https://vectorinstitute.ai/">Vector Institute</a>, <a href="https://rvl.cs.toronto.edu/">Robot Vision and Learning Lab</a> | Advised by <a href="http://www.cs.toronto.edu/~florian/">Prof. Florian Shkurti</a>  
              <br>
                <a href="https://web.cs.toronto.edu/">Department of Computer Science</a>, University of Toronto
              <br>
                May 2020 - Apr 2021 | Toronto, ON
              <p>
                Research in artificial intelligence and robotics. Topics include task-driven perception via learning map representations for downstream 
                control tasks with graph neural networks, and visual state abstraction for Deep Reinforcement Learning based self-driving control.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/google_logo.png" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Software Engineering Intern</papertitle>
              <br>
                <a href="https://about.google/">Google</a>, Cloud
              <br>
                May 2020 - Aug 2020 | San Francisco, CA
              <p> 
                Designed a Proxy-Wasm ABI <a href="https://github.com/proxy-wasm/test-framework">Test Harness and Simulator</a> that supports both 
                low-level and high-level mocking of interactions between a Proxy-Wasm extension and a simulated host environment, 
                allowing developers to test plugins in a safe and controlled environment.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/mcgill_logo.png" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Robotics & ML Research Intern</papertitle>
              <br>
                <a href="https://www.cim.mcgill.ca/~mrl/">Mobile Robotics Lab</a> | Supervised by <a href="http://www.cim.mcgill.ca/~dmeger/">Prof. David Meger</a>, <a href="http://www.cim.mcgill.ca/~dudek/">Prof. Gregory Dudek</a> 
              <br>
                <a href="https://www.cs.mcgill.ca/">School of Computer Science</a>, McGill University
              <br>
                Jan 2020 - May 2020 | Toronto, ON
              <p> 
                Machine learning and robotics research on the topics of Visual SLAM and Deep Reinforcement Learning in collaboration with the Mobile Robotics Lab.
              </p> 
            </td>
          </tr>
        </tbody></table>
      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/huawei_logo.jpg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Deep Learning Research Intern</papertitle>
              <br>
                Huawei Technologies, <a href="http://www.noahlab.com.hk/#/home">Noah's Ark Research Lab</a>
              <br>
                May 2019 - May 2020 | Toronto, ON
              <p> 
                Research and development for autonomous systems (self-driving technology). Research focus and related topics: 2D/3D semantic scene completion, 
                LiDAR-based segmentation, road estimation, visual odometry, depth estimation, and learning-based localization. 
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/autoronto_logo.png" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Autonomy Engineer - Object Detection</papertitle>
              <br>
                <a href="https://www.autodrive.utoronto.ca/about-us">aUToronto</a>, Object Detection Team | <a href="https://www.sae.org/attend/student-events/autodrive-challenge/">SAE/GM Autodrive Challenge </a> 
              <br>
                Aug 2019 - Apr 2020 | Toronto, ON
              <p> 
                Developed a state-of-the-art deep learning pipeline for real-time 3D detection and tracking of vehicles, pedestrians and cyclists from multiple sensor input. 
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/school/UofT-Crest-Square.png" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Robotics Research Intern</papertitle>
              <br>
                <a href="http://asblab.mie.utoronto.ca/">Autonomous Systems and Biomechatronics Lab</a> | Advised by <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Prof. Goldie Nejat</a>
              <br>
                <a href="https://www.mie.utoronto.ca/">Department of Mechanical and Industrial Engineering</a>, University of Toronto
              <br>
                May 2018 - Aug 2018 | Toronto, ON
              <p> 
                Search and rescue robotics - research on the topics of Deep Reinforcement Learning and Transfer Learning for autonomous robot navigation in rough and 
                hazardous terrain. ROS (Robot Operating System) software development for various mobile robots.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/ge_logo.jpg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Software Engineering Intern</papertitle>
              <br>
                General Electric, <a href="https://www.gegridsolutions.com/index.htm">Grid Solutions</a>
              <br>
                May 2017 - Aug 2017 | Markham, ON
              <p> 
                Created customer-end software tools used to accelerate the transition/setup process of new protection and control systems upon upgrade. 
                Designed the current Install-Base and Firmware Revision History databases used by GE internal service teams.
              </p> 
            </td>
          </tr>
        </tbody></table>
        </div>
        <p></p>


        <!-- Projects -->
        <button class="accordion">..Projects and Competitions</button>
        <div class="panel">
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Projects and Competitions</heading>
              <p>
                <strong>Learn by doing</strong> - I've had the opportunity to work on many interesting projects that range across industries such as Robotics, Health Care, Finance, Transportation, and Logistics.  
              <p>
                Links to the source code are embedded in the project titles.
              </p>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p style="text-align:center">
                <em>“Give the pupils something to do, not something to learn;<br>
                  and the doing is of such a nature as to demand thinking; learning naturally results.”</em> - John Dewey
              </p>
              <p>
                <strong>I've worked on a number of exciting software, machine learning, and deep learning projects.</strong>
                Their applications cover a range of industries: Robotics, Graphics, Health Care, Finance, Transportation, Logistics, to name a few! 
              </p>
              <p>
                <strong>The majority of these projects were accomplished in teams!</strong>
                The results also reflect the efforts of the many talented individuals I've had the opportunity to collaborate with and learn from over the years.
              </p> 
              <p>
                Links to the source code are embedded in the project titles.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/baby_ai.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://github.com/agiachris/SfMLearnerMars"> -->
                <papertitle>Instruction Prediction as a Constructive Task for Imitation and Adaptation</papertitle>
              <!-- </a> -->
              <br>
                Stanford University, CS330 Deep Multi-task and Meta Learning
              <p>
                Can natural language substitute as abstract planning medium for solving long-horizon tasks when obtaining
                additional demonstrations is prohibitively expensive?  We show: (a) policies trained to predict actions 
                and instructions (multi-task) improves performance by 30%; (b) policies can be adapted to novel tasks 
                (meta learning) solely from language instructions.
                <a href="https://drive.google.com/file/d/1wVhueOz0Qrlc9onVE2dr3jk50OqPFF-G/view?usp=sharing">Project report</a> /
                <a href="https://drive.google.com/file/d/1Ovui1eG_EaellsExHrko50hs_WMIDr-Q/view?usp=sharing">Poster</a>               
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/StyleGAN-faces.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://github.com/agiachris/SfMLearnerMars"> -->
                <papertitle>Controllable and Image-Free StyleGAN Retraining for Expansive Domain Transfer</papertitle>
              <!-- </a> -->
              <br>
                Stanford University, CS348i Computer Graphics in the Era of AI
              <p>
                <a href="https://arxiv.org/abs/1812.04948">StyleGAN</a> has a remarkable capacity to generate photrealistic images 
                in a controllable manner thanks to its <em>disentangled latent space</em>. However, such architectures can be difficult 
                and costly to train, and domain adaptation methods tend to forego sample diversity and image quality. We prescribe a set
                of ammendments to <a href="https://arxiv.org/abs/2108.00946"></a>StyleGAN-NADA which improve on the pitfalls of text-driven 
                (image-free) domain adaptation of pretrained StyleGANs.  
                <a href="https://drive.google.com/file/d/1ZOJaeT0IWfElH7rWrRxHTHHBgKr1YGKN/view?usp=sharing">Project report</a> /
                <a href="https://drive.google.com/file/d/1y0zCOB8lIUdrqiqCtwQEZD_hV6phjZCC/view?usp=sharing">Presentation</a>               
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/uncertainty_fig.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://github.com/agiachris/SfMLearnerMars"> -->
                <papertitle>Bayesian Temporal Convolutional Networks</papertitle>
              <!-- </a> -->
              <br>
                University of Toronto, CSC413 Neural Networks and Deep Learning
              <p>
                In this project, we explore the application of variational inference via <a href="https://arxiv.org/abs/1505.05424">Bayes by Backprop</a> to the increasingly 
                popular temporal convolutional networks (<a href="https://arxiv.org/abs/1803.01271">TCNs</a>) architecture for time series predictive forecasting. 
                Comparisons are made to the effective state-of-the-art in a series of ablation studies. 
                <a href="https://drive.google.com/file/d/1DZY1iPzOM_QXONoLzzPvMOBEs20Uy_7e/view?usp=sharing">Project report</a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/train_s0_disp.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/SfMLearnerMars">
                <papertitle>SfMLearner on Mars</papertitle>
              </a>
              <br>
                University of Toronto, ROB501 Computer Vision for Robotics
              <p> 
                Adapted the SfMLearner framework from <a href="https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/">Unsupervised Learning of Depth and Ego-Motion from Video</a> 
                to The Canadian Planetary Emulation Terrain Energy-Aware Rover Navigation Dataset (<a href="https://starslab.ca/enav-planetary-dataset/">dataset webpage</a>), 
                and evaluated its feasibility for tracking in low-textured martian-like environments from monochrome image sequences. 
                <a href="https://drive.google.com/file/d/16v0W1VfNscWW1BTFe7GG9-p4JagS2nBy/view?usp=sharing">Project report</a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/rec_chair.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/rotational3DCNN">
                <papertitle>3D Shape Reconstruction</papertitle>
              </a>
              <br>
                University of Toronto, APS360 Applied Fundamentals of Machine Learning
              <p> 
                An empirical study of various 3D Convolutional Neural Network architectures for predicting the full voxel geometry of objects given their partial signed distance 
                field encodings (from the <a href="https://shapenet.org/">ShapeNetCore</a> database). 
                <a href="https://github.com/agiachris/rotational3DCNN/blob/main/project_description_and_results/ProjectReport.pdf">Project report</a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/aer201.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/AER201-Microcontroller">
                <papertitle>Autonomous Packing Robot</papertitle>
              </a>
              <br>
                University of Toronto, AER201 Robot Competition
              <p> 
                Designed, built, and programmed a robot that systematically sorts and packs up to 50 pills/minute to assist those suffering from dimentia. 
                An efficient user interface was created to allow a user to input packing instructions. <strong><em> Team placed 3rd/50.</em></strong> 
                <a href="https://drive.google.com/file/d/1wl2uyzpLt61S0hzdNPHVLlGFJeSz2zRs/view?usp=sharing">Detailed project documentation</a> / 
                <a href="https://www.youtube.com/watch?v=iv9r8VIvHpQ">Youtube video</a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/cec.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/kimwoo11/cec2019">
                <papertitle>Automated Robotic Garbage Collection</papertitle>
              </a>
              <br>
                Canadian Engineering Competition 2019, Programming Challenge
              <p> 
                Based on the robotics <a href="https://en.wikipedia.org/wiki/Sense_Plan_Act">Sense-Plan-Act Paradigm</a>, we created an AI program 
                to handle high-level (path planning, goal setting) and low-level (path following, object avoidance, action execution) tasks for an 
                automated waste collection system to be used in fast food restaurants. <strong><em>4th place Canada.</em></strong> 
                <a href="https://drive.google.com/file/d/1rYYnvsim5CcmZjdqTi77GO5-5UgTqhUc/view?usp=sharing">Presentation </a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/oec_logo.jpg" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/kimwoo11/oec2019">
                <papertitle>Hospital Triage System</papertitle>
              </a>
              <br>
                Ontario Engineering Competition 2019, Programming Challenge
              <p> 
                Developed a machine learning software solution to predict the triage score of emergency patients, allocate available resources to 
                patients, and track key hospital performance metrics to reduce emergency wait times. <strong><em>1st place Ontario.</em></strong> 
                <a href="https://drive.google.com/file/d/131vm1maZUMwwmfP15GKlHtS03BRCOEMn/view?usp=sharing">Presentation</a> / <a href="images/teams/oec_team.jpg">Team photo</a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/utek_logo.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/kimwoo11/utek2019">
                <papertitle>Warehouse Logistics Planning</papertitle>
              </a>
              <br>
                UTEK Engineering Competition 2019, Programming Challenge
              <p> 
                Created a logistics planning algorithm that assigned mobile robots to efficiently retrieve warehouse packages. Our solution combined 
                traditional algorithms such as A* Path Planning with heuristic-based clustering. <strong><em>1st place UofT.</em></strong> 
                <a href="https://drive.google.com/file/d/1ZynKLH1kdHOEQr2_tRnss4r-cF7ILf8S/view?usp=sharing">Presentation</a> / <a href="images/teams/utek_team.jpg">Team photo</a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/mie438.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Smart Intersection - Yonge and Dundas</papertitle>
              <br>
                University of Toronto, MIE438 Robot Design
              <p> 
                We propose a traffic intersection model which uses computer vision to estimate lane congestion and manage traffic flow accordingly. 
                A mockup of our proposal was fabricated to display the behaviour and features of our system. 
                <a href="https://drive.google.com/file/d/10SHGcUwMIGsUryGMhp0yHSlfHsys0UA0/view?usp=sharing">Detailed report</a> / 
                <a href="https://www.youtube.com/watch?v=fOmKNn2y-C4">YouTube video</a> 
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/cibc_logo.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/CIBC-Hackathon">
                <papertitle>Insurance Fraud Detection</papertitle>
              </a>
              <br>
                CIBC Data Studio Hackathon, Programming Challenge
              <p> 
                Developed an unsupervised learning system utilizing Gaussian Mixture Models to identify insurance claim anomalies for CIBC.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/bluesky_logo.jpg" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/Solar-Array-Simulator">
                <papertitle>Solar Array Simulation</papertitle>
              </a>
              <br>
                Blue Sky Solar Racing, Strategic Planning Team
              <p> 
                Created a simulator that ranks the performance of any solar array CAD model by predicting the instantaneous energy generated under various daylight conditions. 
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/gomoku.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/Gomoku-AI-Engine">
                <papertitle>Gomoku AI Engine</papertitle>
              </a>
              <br>
                University of Toronto, Class Competition
              <p> 
                Developed an AI program capable of playing Gomoku against both human and virtual opponents. The software's decision making process 
                is determined by experimentally tuned heuristics which were designed to emulate that of a human opponent. 
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/semsim.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/Semantic-Similarity">
                <papertitle>Word Pairing - Semantic Similarity</papertitle>
              </a>
              <br>
                University of Toronto, Class Competition
              <p> 
                Programmed an intelligent system that approximates the semantic similarity between any two pair of words by parsing data from 
                large novels and computing cosine similarities and Euclidean spaces between vector descriptors of each word.
              </p> 
            </td>
          </tr>
        </tbody></table>
        </div>

      </td>
      </tr>
    </table>


    <script>
      var acc = document.getElementsByClassName("accordion");
      var i;

      for (i = 0; i < acc.length; i++) {
        acc[i].addEventListener("click", function() {
          this.classList.toggle("active");
          var panel = this.nextElementSibling;
          if (panel.style.maxHeight) {
            panel.style.maxHeight = null;
          } else {
            panel.style.maxHeight = panel.scrollHeight + "px";
          } 
        });
      }
    </script>

    <script>
      var coll = document.getElementsByClassName("collapsible");
      var i;

      for (i = 0; i < coll.length; i++) {
        coll[i].addEventListener("click", function() {
          this.classList.toggle("active");
          var content = this.nextElementSibling;
          if (content.style.display === "block") {
            content.style.display = "none";
          } else {
            content.style.display = "block";
          }
        });
      }
    </script>

    <!-- <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
    <div id="particles-js"></div>
    <script>
      particlesJS.load("particles-js", "particles_config/particlesjs-config-gray.json",
      function(){
          console.log("particles.json loaded...")
      })
    </script> -->

  </body>
</html>
