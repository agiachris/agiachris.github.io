<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Christopher Agia">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Chris Agia - Robotics and Learning</title>

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="stylesheet" type="text/css" href="main.css">
    <link rel="icon" type="image/png" href="images/ml_icon.png">

    <style>
      .accordion {
        background-color: #eee;
        color: #444;
        cursor: pointer;
        padding: 18px;
        width: 100%;
        border: none;
        text-align: right;
        outline: none;
        font-size: 22px;
        font-family: monospace;
        transition: 0.4s;
      }

      .active, .accordion:hover {
        background-color: #ccc;
      }

      .panel {
        padding: 0 0px;
        width:100%;
        vertical-align:middle;
        background-color: #f9f9f9;
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.4s ease-out;
      }
      
      /* Style the button that is used to open and close the collapsible content */
      .collapsible {
        background-color: #f9f9f9;
        /* color: #444; */
        cursor: pointer;
        /* padding: 18px; */
        width: 100%;
        border: none;
        text-align: right;
        outline: none;
        /* font-size: 15px; */
        font-family: monospace;
        transition: 0.4s;
      }

      /* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
      .active, .collapsible:hover {
        background-color: #ccc;
      }

      /* Style the collapsible content. Note: hidden by default */
      .content {
        /* padding: 0 18px; */
        display: none;
        overflow: hidden;
        background-color: #f9f9f9;
        transition: max-height 0.4s ease-out;
      }
    </style>  
  </head>

  <body>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
      <tr style="padding:0px">
      <td style="padding:0px">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr style="padding:0px">

            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/headshots/chrisagia_circle_2023.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/headshots/chrisagia_circle_2023.png" class="hoverZoomLink"></a>
            </td>

            <td style="padding:2.5%;width:63%;vertical-align:middle;background-color: #f9f9f9">
              <p style="text-align:center">
                <name>Chris Agia</name>
              </p>
              <p>
                I am a 3rd-year PhD student in Computer Science at Stanford University advised by <a href="https://web.stanford.edu/~bohg/"><strong>Jeannette Bohg</strong></a>
                and <a href="https://web.stanford.edu/~pavone/"><strong>Marco Pavone</strong></a>. I'm a member of the 
                <a href="https://ai.stanford.edu/">Stanford Artificial Intelligence Laboratory</a>, the <a href="https://crfm.stanford.edu/">Center for Research on Foundation Models</a>,
                and work jointly with the <a href="http://iprl.stanford.edu/">Interactive Perception and Robot Learning Lab</a> and the <a href="https://stanfordasl.github.io/">Autonomous Systems Lab</a>.
              </p>
              <p>
                <strong>My research investigates how robots can feasibly and efficiently plan for any task through learning.</strong>
                This requires accurate reasoning over the effects of robot actions into the future, at both a physical and abstract level, just as we do. 
                I'm interested in defining <em>the right substrates</em> for geometric-symbolic reasoning, 
                designing tools to construct them from perception and language, and learning to enact plans downstream.
              </p>
              <p>
                <strong>I set aside weekly time for research discussion and mentorship.</strong> If you feel this could be of 
                benefit, please book an open slot <a href="https://calendly.com/agiachris/virtual-research-chat">here</a>.
              </p>
              <p>
                I'm also a <a href="https://clear.ventures/people/">Clear Ventures Deeptech Fellow</a> <strong>interested in startups</strong>.
                Beyond research, I enjoy sports (<a href="https://stanfordclubsports.com/sports/mens-soccer">Stanford Club Men's Soccer</a>) and music!
              </p>

              <button type="button" class="collapsible">..more</button>
              <div class="content">
                <p>
                  I've had the pleasure of working with <a href="https://jiajunwu.com/">Jiajun Wu</a> in my first-year at Stanford.
                  Before that, I graduated from the 
                  <a href="https://orientation.engsci.utoronto.ca/2020/07/08/engsci-majors-robotics-engineering/">Engineering Science, Robotics</a> 
                  program at the University of Toronto, where I was advised by <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a> 
                  at <a href="https://rvl.cs.toronto.edu/">RVL</a> and the <a href="https://vectorinstitute.ai/">Vector Institute</a>. 
                  I've been fortunate to collaborate with <a href="https://liampaull.ca/">Liam Paull</a> at 
                  <a href="https://montrealrobotics.ca/">MILA</a>, <a href="https://www.cim.mcgill.ca/~dmeger/">David Meger</a> 
                  and <a href="http://www.cim.mcgill.ca/~dudek/">Gregory Dudek</a> at <a href="https://www.cim.mcgill.ca/~mrl/">McGill</a>, 
                  <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Goldie Nejat</a> at the 
                  <a href="http://asblab.mie.utoronto.ca/">University of Toronto</a>, and colleagues from 
                  <a href="https://www.microsoft.com/en-us/research/group/reinforcement-learning-redmond/">Microsoft Research</a> and 
                  <a href="https://ai.facebook.com/">Facebook AI Research</a>. 
                  In industry, I had the opportunity to work on multi-agent reinforcement learning in mixed reality environments at 
                  <a href="https://www.microsoft.com/en-us/research/project/robotics-and-mixed-reality/">Microsoft</a>, perception 
                  and localization for self-driving vehicles at <a href="http://www.noahlab.com.hk/#/home">Huawei Noah's Ark Lab</a> with
                  <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>, and 
                  language-agnostic ABI simulators at <a href="https://about.google/">Google</a> Cloud.
                </p>
              </div>

              <p style="text-align:center">
                cagia[at]cs.stanford.edu &nbsp/&nbsp
                <a href="data/CA_ResearchCV_2024.pdf">CV</a> &nbsp/&nbsp
                <a href="http://www.linkedin.com/in/agiachris/"> LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/agiachris/"> GitHub</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=t8Em5FwAAAAJ&hl=en">Scholar</a>
              </p>

            </td>
          </tr>
          </tbody>
        </table>
        <p></p>

        <!-- Education -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr>
            <td style="text-align: right; background-color: #eee">
              <heading style="font-family: monospace; font-size: 22px; color: #444">..Education</heading>
            </td>
          </tr>
          </tbody>
        </table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>   -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;">
          <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/school/Stanford-Crest-Square.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Ph.D in Computer Science</papertitle>
              <br>
                Department of Computer Science, Stanford University
              <br>
                Sep 2021 | Stanford, CA
              <p> </p> 
              <em><strong>School of Engineering Fellowship</strong></em>
            </td>
          </tr>
          </tbody>
        </table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>   -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;">
          <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/school/UofT-Crest-Square.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>B.A.Sc in Engineering Science, Robotics</papertitle>
              <br>
                Faculty of Applied Science and Engineering, University of Toronto 
              <br>
                Sep 2016 - May 2021 | Toronto, ON
              <p> </p> 
              <em><strong>President's Scholarship Program</strong></em>
              <br>
              <em><strong>NSERC Undergraduate Research Award</strong></em>
              <br>
              <em><strong>Dean's Honour List - 2018-2021</strong></em>
            </td>
          </tr>
        </tbody></table>
        </div>
        <p></p>

        <!-- Research -->
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested bridging concepts from <strong>Robotics</strong>, <strong>Deep Learning</strong>, and <strong>Computer Vision</strong> to build improved task planning, motion planning, decision-making and control systems. More recently, I've explored the use of <strong>unsupervised representation learning</strong> and <strong>reinforcement learning</strong> to create observational / world models that faciliate optimal planning and control. 
              </p>
              <p>
                I've also lead and contributed to perception projects related to: 3D Scene Understanding, 2D/3D Semantic Scene Completion, 2D/3D Object Detection, LiDAR segmentation, and more!
              </p>
            </td>
          </tr>
        </tbody></table> -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="text-align: right; background-color: #eee">
              <heading style="font-family: monospace; font-size: 22px; color: #444">..Research</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p>
                <strong>Core research question.</strong>
              </p>
              <p>
                What unexplored synergies exist across robotics perception & planning, deep learning, and reinforcement learning 
                that would allow us to build faster task & motion planners, more optimal controllers, and safer decision making systems?
              </p>
            </td>
          </tr>
        </tbody></table>

          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Preprints</heading>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Journal Papers</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/t2m_task6.gif" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Text2Motion: From Natural Language Instructions to Feasible Plans</papertitle>
              <br>
              <a href="https://kevin-thankyou-lin.github.io/">Kevin Lin*</a>, <strong>Christopher Agia*</strong>, <a href="https://cs.stanford.edu/~takatoki/">Toki Migimatsu</a>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>, <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>
              <br>
              <em>Special Issue: Large Language Models in Robotics, Autonomous Robots (AR)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.12153">arXiv</a> / <a href="https://sites.google.com/stanford.edu/text2motion">Project Site</a> / <a href="https://link.springer.com/article/10.1007/s10514-023-10131-7">Journal</a>
              <p>Pretrained large language models can be readily used to obtain high-level robot plans from natural lanugage instructions, but should these
                plans be executed without verifying them on the geometric-level? We propose Text2Motion, a language-based planner that tests if 
                LLM-generated plans (a) satisfy user instructions and (b) are geometric feasibility prior to executing them.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/semantic_anomaly.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Semantic Anomaly Detection with Large Language Models</papertitle>
              <br>
              <a href="https://stanfordasl.github.io//people/amine-elhafsi/">Amine Elhafsi</a>, <a href="https://stanfordasl.github.io//people/rohan-sinha/">Rohan Sinha</a>, <strong>Christopher Agia</strong>, <a href="https://stanfordasl.github.io//people/edward-schmerling/">Edward Schmerling</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/issa_nesnas/">Issa A. D. Nesnas</a>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>
              <br>
              <em>Special Issue: Large Language Models in Robotics, Autonomous Robots (AR)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2305.11307">arXiv</a> / <a href="https://sites.google.com/view/llm-anomaly-detection/home">Project Site </a> / <a href="https://link.springer.com/article/10.1007/s10514-023-10132-6">Journal</a>
              <p>System-level failures are not due to failures of any individual component of the autonomy stack but
                system-level deficiencies in semantic reasoning. Such edge cases, dubbed semantic anomalies, 
                are simple for a human to disentangle yet require insightful reasoning. We introduce a runtime monitor based 
                on large language models to recognize failure-inducing semantic anomalies.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/sem_loc.gif" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Lightweight Semantic-aided Localization with Spinning LiDAR Sensor</papertitle>
              <br>
              Yuan Ren, <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>, <a href="https://rancheng.github.io/about/">Ran Cheng</a>, <strong>Christopher Agia</strong>
              <br>
              [Patented]. <em>IEEE Transactions on Intelligent Vehicles (T-IV)</em>, 2021
              <br>
              <a href="papers/SemanticLoc-TIV-2021.pdf">PDF</a> / <a href="https://ieeexplore.ieee.org/document/9495210">IEEExplore</a>
              <p>How can semantic information be leveraged to improve localization accuracy in changing environments? We present a robust LiDAR-based localization 
                algorithm that exploits both semantic and geometric properties of the scene with an adaptive fusion strategy.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" height="140" src="images/research/sim2real_drl_rough_nav.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>A Sim-to-Real Pipeline for Deep Reinforcement Learning for Autonomous Robot Navigation in Cluttered Rough Terrain</papertitle>
              <br>
              <a href="https://www.linkedin.com/in/richardhu-12dsf/?originalSubdomain=ca">Han Hu*</a>, <a href="https://www.linkedin.com/in/kczhang/?originalSubdomain=ca">Kaicheng Zhang*</a>, <a href="https://www.linkedin.com/in/aaron-hao-tan/?originalSubdomain=ca">Aaron Hao Tan</a>, <a href="https://www.linkedin.com/in/michael-ruan-0822/">Michael Ruan</a>, <strong>Christopher Agia</strong>, <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/"> Goldie Nejat</a>
              <br>
              <em>IEEE Robotics and Automation Letters (RA-L)</em> at <em>IROS</em>, 2021 | Prague, CZ
              <br>
              <a href="papers/Sim2RealDRLNav-RAL-2021.pdf">PDF</a> / <a href="https://www.youtube.com/watch?v=dtYlNWvK-7k&ab_channel=AutonomousSystemsandBiomechatronicsLab%28UniversityofToronto%29">Video</a> / <a href="https://ieeexplore.ieee.org/document/9468918">IEEExplore</a>
              <p>Deep Reinforcement Learning is effective for learning robot navigation policies in rough terrain and cluttered simulated environments. 
                In this work, we introduce a series of techniques that are applied in the policy learning phase to enhance transferability to real-world domains.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Conference Papers</heading>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" height="140" src="images/research/aesop_llm.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>Real-Time Anomaly Detection and Reactive Planning with Large Language Models</papertitle>
              <br>
              <a href="https://stanfordasl.github.io//people/rohan-sinha/">Rohan Sinha</a>, <a href="https://stanfordasl.github.io//people/amine-elhafsi/">Amine Elhafsi</a>, <strong>Christopher Agia</strong>, <a href="https://stanfordasl.github.io//people/edward-schmerling/">Edward Schmerling</a>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2024 | Delft, Netherlands
              <br>
              <span style="color:red;">Outstanding Paper Award</span>
              <br>
              <a href="https://arxiv.org/abs/2407.08735v1">arXiv</a> / <a href="https://sites.google.com/view/aesop-llm">Project Site</a> / <a href="https://techxplore.com/news/2024-08-stage-framework-llm-based-anomaly.html">TechXplore</a>
              <p>
                How can we mitigate the computational expense and latency of LLMs for real-time anomaly detection and reactive planning? 
                We propose a two-stage reasoning framework, whereby fast a LLM embedding model flags potential observational anomalies
                while a slower generative LLM assesses the safety-criticality of flagged anomalies and selects a safety-preserving fallback plan.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" height="80" src="images/research/cooking_droid_speed.mp4" type="video/mp4" muted autoplay loop></video>
              <video width="140" height="80" src="images/research/apple_droid.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset</papertitle>
              <br>
              DROID Dataset Team
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2024 | Delft, Netherlands
              <br>
              <a href="https://arxiv.org/abs/2403.12945">arXiv</a> / <a href="https://droid-dataset.github.io/">Project Site</a> / <a href="https://droid-dataset.github.io/droid/">Documentation</a>
              <p>We introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with
                76k demonstration trajectories (or 350 hours of interaction data), collected across 564 scenes and 86 
                tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate
                that training with DROID leads to policies with higher performance and improved generalization ability.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" height="80" src="images/research/dare_mission_concept.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>Modeling Considerations for Developing Deep Space Autonomous Spacecraft and Simulators</papertitle>
              <br>
                <strong>Christopher Agia</strong>,
                <a href="https://www.guillemc.com/">Guillem Casadesus Vila</a>,
                <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/saptarshi_bandyopadhyay/">Saptarshi Bandyopadhyay</a>,
                <a href="https://scienceandtechnology.jpl.nasa.gov/people/d_bayard">David S. Bayard</a>,
                <a href="https://descanso.jpl.nasa.gov/biography/cheung.html">Kar-Ming Cheung</a>,
                <a href="https://ieeexplore.ieee.org/author/37336718000">Charles H. Lee</a>,
                <a href="https://ieeexplore.ieee.org/author/37285842700">Eric Wood</a>,
                <a href="https://www.linkedin.com/in/ian-aenishanslin-a46b61175/?originalSubdomain=fr">Ian Aenishanslin</a>,
                <a href="https://www.linkedin.com/in/steven-ardito-08a705b/">Steven Ardito</a>,
                <a href="https://www.cs.ucla.edu/lorraine-fesq/">Lorraine Fesq</a>,
                <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>,
                <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/issa_nesnas/">Issa A. D. Nesnas</a>
              <br>
              <em>IEEE Aerospace Conference (AeroConf)</em>, 2024 | Montana, US
              <br>
              <a href="papers/Spacecraft-Models-AeroConf-2024.pdf">PDF</a> / <a href="https://arxiv.org/abs/2401.11371">arXiv</a> / <a href="https://sites.google.com/stanford.edu/spacecraft-models">Project Site</a> / <a href="https://youtu.be/ejP_IDua6J0">YouTube</a>
              <p>
                Future space exploration missions to unknown worlds will require robust reasoning, planning, and decision-making capabilities,
                enabled by the right choice of onboard models. In this work, we aim to understand what onboard models a spacecraft needs for fully autonomous space exploration.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" height="80" src="images/research/openx_teaser.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>Open X-Embodiment: Robotic Learning Datasets and RT-X Models</papertitle>
              <br>
              Open X-Embodiment Collaboration
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024 | Yokohama, Japan
              <br>
              <span style="color:red;">Best Paper Award</span>
              <br>
              <a href="https://arxiv.org/abs/2310.08864">arXiv</a> / <a href="https://robotics-transformer-x.github.io/">Project Site</a> / <a href="https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/">Blogpost</a> / <a href="https://github.com/google-deepmind/open_x_embodiment">Code</a>
              <p>Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. 
                Can we instead train “generalist” X-robot policy that can be adapted efficiently to new robots, tasks, and environments?
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video width="140" height="80" src="images/research/stap_hook_reach.mp4" type="video/mp4" muted autoplay loop></video>
              <video width="140" height="80" src="images/research/stap_constrained_packing.mp4" type="video/mp4" muted autoplay loop></video>
            </td>
            <td width="75%" valign="middle">
              <papertitle>STAP: Sequencing Task-Agnostic Policies</papertitle>
              <br>
              <strong>Christopher Agia*</strong>, <a href="https://cs.stanford.edu/~takatoki/">Toki Migimatsu*</a>, <a href="https://jiajunwu.com/">Jiajun Wu</a>, <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2023 | London, UK
              <br>
              <a href="papers/STAP-ICRA-2023.pdf">PDF</a> / <a href="https://arxiv.org/abs/2210.12250">arXiv</a> / <a href="https://sites.google.com/stanford.edu/stap/home">Project Site</a> / <a href="https://github.com/agiachris/STAP">Code</a>
              <p>Solving sequential manipulation tasks requires coordinating geometric dependencies between actions. 
                We develop a scalable framework for training skills independently, and then combine the skills at planning
                time to solve unseen long-horizon tasks. Planning is formulated as a maximization problem over the expected 
                success of the skill sequence, which we demonstrate is well-approximated by the product of Q-values.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/taskography_3dsg.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Taskography: Evaluating Robot Task Planning over Large 3D Scene Graphs</papertitle>
              <br>
              <strong>Christopher Agia*</strong>, <a href="https://krrish94.github.io/author/krishna-murthy-jatavallabhula/">Krishna Murthy Jatavallabhula*</a>, <a href="https://www.linkedin.com/in/khodeir/?originalSubdomain=ca">Mohamed Khodeir</a>, <a href="https://www.microsoft.com/en-us/research/people/onmiksik/">Ondrej Miksik</a>, <a href="http://www.mustafamukadam.com/">Mustafa Mukadam</a>, <a href="http://vibhavvineet.info/">Vibhav Vineet</a>, <a href="https://liampaull.ca/">Liam Paull</a>, <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a>
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2021 | London, UK
              <br>
              <a href="papers/Taskography-CoRL-2021.pdf">PDF</a> / <a href="papers/Taskography-Poster-CoRL-2021.pdf">Poster</a> / <a href="https://arxiv.org/abs/2207.05006">arXiv</a> / <a href="https://taskography.github.io/">Project Site</a> / <a href="https://github.com/taskography">Code</a>
              <p>3D Scene Graphs (3DSGs) are informative abstractions of our world that unify symbolic, semantic, and metric scene representations. 
                We present a benchmark for robot task planning over large 3DSGs and evaluate classical and learning-based planners; 
                showing that real-time planning requires 3DSGs and planners to be jointly adapted to better exploit 3DSG hierarchies.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/aug_att_rl.gif" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Latent Attention Augmentation for Robust Autonomous Driving Policies</papertitle>
              <br>
              <a href="https://rancheng.github.io/about/">Ran Cheng*</a>, <strong>Christopher Agia*</strong>, <a href="https://www.cim.mcgill.ca/~dmeger/">David Meger</a>, <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a>, <a href="http://www.cim.mcgill.ca/~dudek/">Gregory Dudek</a>
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2021 | Prague, CZ
              <br>
              <a href="papers/AttAugDRL-Self-Driving-IROS-2021.pdf">PDF</a> / <a href="https://ieeexplore.ieee.org/document/9636449">IEEExplore</a>
              <p>Pretraining visual representations for robotic reinforcement learning can improve sample efficiency and policy performance. 
                In this paper, we take an alternate approach and propose to augment the state embeddings of a self-driving agent with 
                attention in the latent space, accelerating the convergence of Actor-Critic algorithms.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/s3cnet_demo.gif" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point Clouds</papertitle>
              <br>
              <a href="https://rancheng.github.io/about/">Ran Cheng*</a>, <strong>Christopher Agia*</strong>, Yuan Ren, <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2020 | Cambridge, US
              <br> 
              <a href="papers/S3CNet-CoRL-2020.pdf">PDF</a> / <a href="https://www.youtube.com/watch?v=ircJBFc5PqM&feature=youtu.be&ab_channel=CoRLConference">Talk</a> / <a href="https://www.youtube.com/watch?v=voU_zAhNDnQ&feature=youtu.be&ab_channel=RanCheng">Video</a> / <a href="https://arxiv.org/abs/2012.09242">arXiv</a>
              <p>Small-scale semantic reconstruction methods have had little success in large outdoor scenes as a result of exponential increases in sparsity, 
                and a computationally expensive design. We propose a sparse convolutional network architecture based on the <a href="https://github.com/StanfordVL/MinkowskiEngine">Minkowski Engine</a>, 
                achieving state-of-the-art results for semantic scene completion in 2D/3D space from LiDAR point clouds.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/ddvo_res.gif" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Depth Prediction for Monocular Direct Visual Odometry</papertitle>
              <br>
              <a href="https://rancheng.github.io/about/">Ran Cheng</a>, <strong>Christopher Agia</strong>, <a href="https://www.cim.mcgill.ca/~dmeger/">David Meger</a>, <a href="http://www.cim.mcgill.ca/~dudek/">Gregory Dudek</a>
              <br>
              <em>Conference on Computer and Robotic Vision (CRV)</em>, 2020 | Ottawa, CA
              <br> 
              <a href="papers/CNN-DVO-CRV-2020.pdf">PDF</a> / <a href="https://www.youtube.com/watch?v=Z8vpet0doik&feature=youtu.be&ab_channel=ComputerRobotVision">Talk</a> / <a href="papers/CNN-DVO-Poster-CRV-2020.pdf">Poster</a> / <a href="https://ieeexplore.ieee.org/document/9108693">IEEExplore</a>
              <p>Direct methods are able to track motion with considerable long-term accuracy. However, scale inconsistent estimates arise from random or unit depth initialization. 
                We integrate dense depth prediction with the Direct Sparse Odometry system to accelerate convergence in the windowed bundle-adjustment and promote estimates with consistent scale.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Theses</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/thesis_3dsg.gif" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Contextual Graph Representations for Task-driven 3D Perception and Planning</papertitle>
              <br>
              <strong>Christopher Agia</strong>, <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a>
              <br>
              <em>Division of Engineering Science, University of Toronto</em>, 2020 | Toronto, CA
              <br>
              <a href="https://drive.google.com/file/d/1LjTdgwuiJa-gIiVbbqj9vh-qoEZgqkb_/view?usp=sharing">PDF</a>
              <p>We evaluate the suitability of existing simulators for research at the intersection of task planning and 3D scene graphs and
                construct a benchmark for comparison of symbolic planners. Furthermore, we explore the use of Graph Neural Networks to
                harness invariances in the relational structure of planning domains and learn representations that afford faster planning.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Patents</heading>
              <p>Several components of my industry research projects were patented alongside submitting to conference / journal venues.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/patent_road_segmentation.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Systems and Methods for Generating a Road Surface Semantic Segmentation Map from a Sequence of Point Clouds</papertitle>
              <br>
              <strong>Christopher Agia</strong>, <a href="https://rancheng.github.io/about/">Ran Cheng</a>, Yuan Ren, <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>
              <br>
              Application No. 17/676,131. U.S. Patent and Trademark Office, 2022
              <p>Relates to processing point clouds for autonomous driving of a vehicle. More specifically, relates to processing a sequence of point 
                clouds to generate a birds-eye-view (BEV) image of an environment of the vehicle which includes pixels associated with road surface labels.</p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;background-color: #f9f9f9;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/research/patent_scene_completion.png" alt="clean-usnob" width="140" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Methods and Systems for Semantic Scene Completion for Sparse 3D Data</papertitle>
              <br>
              <a href="https://rancheng.github.io/about/">Ran Cheng*</a>, <strong>Christopher Agia*</strong>, Yuan Ren, <a href="https://scholar.google.ca/citations?user=-rCulKwAAAAJ&hl=en">Bingbing Liu</a>
              <br>
              Application No. 17/492,261. U.S. Patent and Trademark Office, 2022
              <p>Relates to methods and systems for generating semantically completed 3D data from sparse 3D data such as point clouds.</p>
            </td>
          </tr>
        </tbody></table>
        <p></p>

        
        <!-- Work Experience -->
        <button class="accordion">..Work Experience</button>
        <div class="panel">
        <p></p>
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work Experience</heading>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/NASA_logo.jpg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Visiting Research Scholar</papertitle>
              <br>
                <a href="https://www.jpl.nasa.gov/">NASA Jet Propulsion Laboratory</a>, <a href="https://www-robotics.jpl.nasa.gov/">Mobility and Robotics Systems</a>
              <br>
                Jun 2023 - Sep 2023 | Pasadena, California
              <p> 
                Research on deep space robotic autonomy.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/microsoft_logo.jpg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Software Engineering Intern</papertitle>
              <br>
                <a href="https://www.microsoft.com/en-us/research/project/robotics-and-mixed-reality/">Microsoft</a>, Mixed Reality and Robotics
              <br>
                May 2021 - Aug 2021 | Redmond, Washington
              <p> 
                Research & development at the intersection of mixed reality, artificial intelligence, and robotics. 
                Created a process unlocking the training and <a href="https://www.microsoft.com/en-us/hololens/hardware">HL2</a> 
                deployment of multi-agent reinforcement learning scenarios in shared digital spatial-semantic representations with 
                <a href="https://docs.microsoft.com/en-us/windows/mixed-reality/design/scene-understanding">Scene Understanding</a>.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/vector_institute_logo.jpg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Robotics & ML Researcher</papertitle>
              <br>
                <a href="https://vectorinstitute.ai/">Vector Institute</a>, <a href="https://rvl.cs.toronto.edu/">Robot Vision and Learning Lab</a> | Advised by <a href="http://www.cs.toronto.edu/~florian/">Prof. Florian Shkurti</a>  
              <br>
                <a href="https://web.cs.toronto.edu/">Department of Computer Science</a>, University of Toronto
              <br>
                May 2020 - Apr 2021 | Toronto, ON
              <p>
                Research in artificial intelligence and robotics. Topics include task-driven perception via learning map representations for downstream 
                control tasks with graph neural networks, and visual state abstraction for Deep Reinforcement Learning based self-driving control.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/google_logo.png" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Software Engineering Intern</papertitle>
              <br>
                <a href="https://about.google/">Google</a>, Cloud
              <br>
                May 2020 - Aug 2020 | San Francisco, CA
              <p> 
                Designed a Proxy-Wasm ABI <a href="https://github.com/proxy-wasm/test-framework">Test Harness and Simulator</a> that supports both 
                low-level and high-level mocking of interactions between a Proxy-Wasm extension and a simulated host environment, 
                allowing developers to test plugins in a safe and controlled environment.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/mcgill_logo.png" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Robotics & ML Research Intern</papertitle>
              <br>
                <a href="https://www.cim.mcgill.ca/~mrl/">Mobile Robotics Lab</a> | Supervised by <a href="http://www.cim.mcgill.ca/~dmeger/">Prof. David Meger</a>, <a href="http://www.cim.mcgill.ca/~dudek/">Prof. Gregory Dudek</a> 
              <br>
                <a href="https://www.cs.mcgill.ca/">School of Computer Science</a>, McGill University
              <br>
                Jan 2020 - May 2020 | Toronto, ON
              <p> 
                Machine learning and robotics research on the topics of Visual SLAM and Deep Reinforcement Learning in collaboration with the Mobile Robotics Lab.
              </p> 
            </td>
          </tr>
        </tbody></table>
      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/huawei_logo.jpg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Deep Learning Research Intern</papertitle>
              <br>
                Huawei Technologies, <a href="http://www.noahlab.com.hk/#/home">Noah's Ark Research Lab</a>
              <br>
                May 2019 - May 2020 | Toronto, ON
              <p> 
                Research and development for autonomous systems (self-driving technology). Research focus and related topics: 2D/3D semantic scene completion, 
                LiDAR-based segmentation, road estimation, visual odometry, depth estimation, and learning-based localization. 
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/autoronto_logo.png" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Autonomy Engineer - Object Detection</papertitle>
              <br>
                <a href="https://www.autodrive.utoronto.ca/about-us">aUToronto</a>, Object Detection Team | <a href="https://www.sae.org/attend/student-events/autodrive-challenge/">SAE/GM Autodrive Challenge </a> 
              <br>
                Aug 2019 - Apr 2020 | Toronto, ON
              <p> 
                Developed a state-of-the-art deep learning pipeline for real-time 3D detection and tracking of vehicles, pedestrians and cyclists from multiple sensor input. 
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/school/UofT-Crest-Square.png" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Robotics Research Intern</papertitle>
              <br>
                <a href="http://asblab.mie.utoronto.ca/">Autonomous Systems and Biomechatronics Lab</a> | Advised by <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Prof. Goldie Nejat</a>
              <br>
                <a href="https://www.mie.utoronto.ca/">Department of Mechanical and Industrial Engineering</a>, University of Toronto
              <br>
                May 2018 - Aug 2018 | Toronto, ON
              <p> 
                Search and rescue robotics - research on the topics of Deep Reinforcement Learning and Transfer Learning for autonomous robot navigation in rough and 
                hazardous terrain. ROS (Robot Operating System) software development for various mobile robots.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experience/ge_logo.jpg" alt="clean-usnob" width="120" height="120">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Software Engineering Intern</papertitle>
              <br>
                General Electric, <a href="https://www.gegridsolutions.com/index.htm">Grid Solutions</a>
              <br>
                May 2017 - Aug 2017 | Markham, ON
              <p> 
                Created customer-end software tools used to accelerate the transition/setup process of new protection and control systems upon upgrade. 
                Designed the current Install-Base and Firmware Revision History databases used by GE internal service teams.
              </p> 
            </td>
          </tr>
        </tbody></table>
        </div>
        <p></p>


        <!-- Projects -->
        <button class="accordion">..Projects and Competitions</button>
        <div class="panel">
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Projects and Competitions</heading>
              <p>
                <strong>Learn by doing</strong> - I've had the opportunity to work on many interesting projects that range across industries such as Robotics, Health Care, Finance, Transportation, and Logistics.  
              <p>
                Links to the source code are embedded in the project titles.
              </p>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p style="text-align:center">
                <em>“Give the pupils something to do, not something to learn;<br>
                  and the doing is of such a nature as to demand thinking; learning naturally results.”</em> - John Dewey
              </p>
              <p>
                <strong>I've worked on a number of exciting software, machine learning, and deep learning projects.</strong>
                Their applications cover a range of industries: Robotics, Graphics, Health Care, Finance, Transportation, Logistics, to name a few! 
              </p>
              <p>
                <strong>The majority of these projects were accomplished in teams!</strong>
                The results also reflect the efforts of the many talented individuals I've had the opportunity to collaborate with and learn from over the years.
              </p> 
              <p>
                Links to the source code are embedded in the project titles.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/baby_ai.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://github.com/agiachris/SfMLearnerMars"> -->
                <papertitle>Instruction Prediction as a Constructive Task for Imitation and Adaptation</papertitle>
              <!-- </a> -->
              <br>
                Stanford University, CS330 Deep Multi-task and Meta Learning
              <p>
                Can natural language substitute as abstract planning medium for solving long-horizon tasks when obtaining
                additional demonstrations is prohibitively expensive?  We show: (a) policies trained to predict actions 
                and instructions (multi-task) improves performance by 30%; (b) policies can be adapted to novel tasks 
                (meta learning) solely from language instructions.
                <a href="https://drive.google.com/file/d/1wVhueOz0Qrlc9onVE2dr3jk50OqPFF-G/view?usp=sharing">Project report</a> /
                <a href="https://drive.google.com/file/d/1Ovui1eG_EaellsExHrko50hs_WMIDr-Q/view?usp=sharing">Poster</a>               
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/StyleGAN-faces.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://github.com/agiachris/SfMLearnerMars"> -->
                <papertitle>Controllable and Image-Free StyleGAN Retraining for Expansive Domain Transfer</papertitle>
              <!-- </a> -->
              <br>
                Stanford University, CS348i Computer Graphics in the Era of AI
              <p>
                <a href="https://arxiv.org/abs/1812.04948">StyleGAN</a> has a remarkable capacity to generate photrealistic images 
                in a controllable manner thanks to its <em>disentangled latent space</em>. However, such architectures can be difficult 
                and costly to train, and domain adaptation methods tend to forego sample diversity and image quality. We prescribe a set
                of ammendments to <a href="https://arxiv.org/abs/2108.00946"></a>StyleGAN-NADA which improve on the pitfalls of text-driven 
                (image-free) domain adaptation of pretrained StyleGANs.  
                <a href="https://drive.google.com/file/d/1ZOJaeT0IWfElH7rWrRxHTHHBgKr1YGKN/view?usp=sharing">Project report</a> /
                <a href="https://drive.google.com/file/d/1y0zCOB8lIUdrqiqCtwQEZD_hV6phjZCC/view?usp=sharing">Presentation</a>               
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/uncertainty_fig.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://github.com/agiachris/SfMLearnerMars"> -->
                <papertitle>Bayesian Temporal Convolutional Networks</papertitle>
              <!-- </a> -->
              <br>
                University of Toronto, CSC413 Neural Networks and Deep Learning
              <p>
                In this project, we explore the application of variational inference via <a href="https://arxiv.org/abs/1505.05424">Bayes by Backprop</a> to the increasingly 
                popular temporal convolutional networks (<a href="https://arxiv.org/abs/1803.01271">TCNs</a>) architecture for time series predictive forecasting. 
                Comparisons are made to the effective state-of-the-art in a series of ablation studies. 
                <a href="https://drive.google.com/file/d/1DZY1iPzOM_QXONoLzzPvMOBEs20Uy_7e/view?usp=sharing">Project report</a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/train_s0_disp.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/SfMLearnerMars">
                <papertitle>SfMLearner on Mars</papertitle>
              </a>
              <br>
                University of Toronto, ROB501 Computer Vision for Robotics
              <p> 
                Adapted the SfMLearner framework from <a href="https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/">Unsupervised Learning of Depth and Ego-Motion from Video</a> 
                to The Canadian Planetary Emulation Terrain Energy-Aware Rover Navigation Dataset (<a href="https://starslab.ca/enav-planetary-dataset/">dataset webpage</a>), 
                and evaluated its feasibility for tracking in low-textured martian-like environments from monochrome image sequences. 
                <a href="https://drive.google.com/file/d/16v0W1VfNscWW1BTFe7GG9-p4JagS2nBy/view?usp=sharing">Project report</a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/rec_chair.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/rotational3DCNN">
                <papertitle>3D Shape Reconstruction</papertitle>
              </a>
              <br>
                University of Toronto, APS360 Applied Fundamentals of Machine Learning
              <p> 
                An empirical study of various 3D Convolutional Neural Network architectures for predicting the full voxel geometry of objects given their partial signed distance 
                field encodings (from the <a href="https://shapenet.org/">ShapeNetCore</a> database). 
                <a href="https://github.com/agiachris/rotational3DCNN/blob/main/project_description_and_results/ProjectReport.pdf">Project report</a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/aer201.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/AER201-Microcontroller">
                <papertitle>Autonomous Packing Robot</papertitle>
              </a>
              <br>
                University of Toronto, AER201 Robot Competition
              <p> 
                Designed, built, and programmed a robot that systematically sorts and packs up to 50 pills/minute to assist those suffering from dimentia. 
                An efficient user interface was created to allow a user to input packing instructions. <strong><em> Team placed 3rd/50.</em></strong> 
                <a href="https://drive.google.com/file/d/1wl2uyzpLt61S0hzdNPHVLlGFJeSz2zRs/view?usp=sharing">Detailed project documentation</a> / 
                <a href="https://www.youtube.com/watch?v=iv9r8VIvHpQ">Youtube video</a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/cec.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/kimwoo11/cec2019">
                <papertitle>Automated Robotic Garbage Collection</papertitle>
              </a>
              <br>
                Canadian Engineering Competition 2019, Programming Challenge
              <p> 
                Based on the robotics <a href="https://en.wikipedia.org/wiki/Sense_Plan_Act">Sense-Plan-Act Paradigm</a>, we created an AI program 
                to handle high-level (path planning, goal setting) and low-level (path following, object avoidance, action execution) tasks for an 
                automated waste collection system to be used in fast food restaurants. <strong><em>4th place Canada.</em></strong> 
                <a href="https://drive.google.com/file/d/1rYYnvsim5CcmZjdqTi77GO5-5UgTqhUc/view?usp=sharing">Presentation </a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/oec_logo.jpg" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/kimwoo11/oec2019">
                <papertitle>Hospital Triage System</papertitle>
              </a>
              <br>
                Ontario Engineering Competition 2019, Programming Challenge
              <p> 
                Developed a machine learning software solution to predict the triage score of emergency patients, allocate available resources to 
                patients, and track key hospital performance metrics to reduce emergency wait times. <strong><em>1st place Ontario.</em></strong> 
                <a href="https://drive.google.com/file/d/131vm1maZUMwwmfP15GKlHtS03BRCOEMn/view?usp=sharing">Presentation</a> / <a href="images/teams/oec_team.jpg">Team photo</a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/utek_logo.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/kimwoo11/utek2019">
                <papertitle>Warehouse Logistics Planning</papertitle>
              </a>
              <br>
                UTEK Engineering Competition 2019, Programming Challenge
              <p> 
                Created a logistics planning algorithm that assigned mobile robots to efficiently retrieve warehouse packages. Our solution combined 
                traditional algorithms such as A* Path Planning with heuristic-based clustering. <strong><em>1st place UofT.</em></strong> 
                <a href="https://drive.google.com/file/d/1ZynKLH1kdHOEQr2_tRnss4r-cF7ILf8S/view?usp=sharing">Presentation</a> / <a href="images/teams/utek_team.jpg">Team photo</a>
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/mie438.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Smart Intersection - Yonge and Dundas</papertitle>
              <br>
                University of Toronto, MIE438 Robot Design
              <p> 
                We propose a traffic intersection model which uses computer vision to estimate lane congestion and manage traffic flow accordingly. 
                A mockup of our proposal was fabricated to display the behaviour and features of our system. 
                <a href="https://drive.google.com/file/d/10SHGcUwMIGsUryGMhp0yHSlfHsys0UA0/view?usp=sharing">Detailed report</a> / 
                <a href="https://www.youtube.com/watch?v=fOmKNn2y-C4">YouTube video</a> 
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/cibc_logo.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/CIBC-Hackathon">
                <papertitle>Insurance Fraud Detection</papertitle>
              </a>
              <br>
                CIBC Data Studio Hackathon, Programming Challenge
              <p> 
                Developed an unsupervised learning system utilizing Gaussian Mixture Models to identify insurance claim anomalies for CIBC.
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/bluesky_logo.jpg" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/Solar-Array-Simulator">
                <papertitle>Solar Array Simulation</papertitle>
              </a>
              <br>
                Blue Sky Solar Racing, Strategic Planning Team
              <p> 
                Created a simulator that ranks the performance of any solar array CAD model by predicting the instantaneous energy generated under various daylight conditions. 
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/gomoku.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/Gomoku-AI-Engine">
                <papertitle>Gomoku AI Engine</papertitle>
              </a>
              <br>
                University of Toronto, Class Competition
              <p> 
                Developed an AI program capable of playing Gomoku against both human and virtual opponents. The software's decision making process 
                is determined by experimentally tuned heuristics which were designed to emulate that of a human opponent. 
              </p> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projects/semsim.png" alt="clean-usnob" width="100" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/agiachris/Semantic-Similarity">
                <papertitle>Word Pairing - Semantic Similarity</papertitle>
              </a>
              <br>
                University of Toronto, Class Competition
              <p> 
                Programmed an intelligent system that approximates the semantic similarity between any two pair of words by parsing data from 
                large novels and computing cosine similarities and Euclidean spaces between vector descriptors of each word.
              </p> 
            </td>
          </tr>
        </tbody></table>
        </div>

      </td>
      </tr>
    </table>


    <script>
      var acc = document.getElementsByClassName("accordion");
      var i;

      for (i = 0; i < acc.length; i++) {
        acc[i].addEventListener("click", function() {
          this.classList.toggle("active");
          var panel = this.nextElementSibling;
          if (panel.style.maxHeight) {
            panel.style.maxHeight = null;
          } else {
            panel.style.maxHeight = panel.scrollHeight + "px";
          } 
        });
      }
    </script>

    <script>
      var coll = document.getElementsByClassName("collapsible");
      var i;

      for (i = 0; i < coll.length; i++) {
        coll[i].addEventListener("click", function() {
          this.classList.toggle("active");
          var content = this.nextElementSibling;
          if (content.style.display === "block") {
            content.style.display = "none";
          } else {
            content.style.display = "block";
          }
        });
      }
    </script>

    <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
    <div id="particles-js"></div>
    <script>
      particlesJS.load("particles-js", "particles_config/particlesjs-config-gray.json",
      function(){
          console.log("particles.json loaded...")
      })
    </script>

  </body>
</html>
